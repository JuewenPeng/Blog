---
layout: post
title: "Denoising Difusion Probabilistic Models"
author: "Juewen Peng"
comments: false
tags: [Diffusion Model, Generative Model, Annealed Langevin Dynamics]
excerpt_separator: <!--more-->
sticky: false
hidden: false
katex: true
---

<!-- "highlight language" refer to https://github.com/rouge-ruby/rouge/wiki/List-of-supported-languages-and-lexers -->

So far, many different kinds of generative models have been proposed, such as GAN, VAE, and Flow-based models. They have shown great success in generating high-quality samples, but each has some limitations of its own. <!--more--> GAN models are known for potentially unstable training and less diversity in generation due to their adversarial training nature. VAE relies on a surrogate loss. Flow models have to use specialized architectures to construct reversible transform.

Diffusion models are inspired by non-equilibrium thermodynamics. They define a Markov chain of diffusion steps to slowly add random noise to data and then learn to reverse the diffusion process to construct desired data samples from the noise. Unlike VAE or flow models, diffusion models are learned with a fixed procedure and the latent variable has high dimensionality (same as the original data).

![image]({{site.baseurl}}/images/image-2022-07-02-00-23-31.png){:width="90%"}
<div class="figcap">Figure 1: Overview of different types of generative models.</div>


This article mainly references [https://lilianweng.github.io/posts/2021-07-11-diffusion-models](https://lilianweng.github.io/posts/2021-07-11-diffusion-models) and [https://sunlin-ai.github.io/2022/05/28/DDPM.html](https://sunlin-ai.github.io/2022/05/28/DDPM.html). For clarity, most derivations are omitted. If you are interested in them, please refer to the above two links.

---

<br>

## 1. Principle of DDPM

The concept of "diffusion model" was first proposed by [Sohl-Dickstein et al.](https://arxiv.org/abs/1503.03585) in 2015, and **[Ho et al. (DDPM)](https://arxiv.org/abs/2006.11239) successfully develop it further by simplifing the procedure and presenting high quality image synthesis results.** 

### 1.1. Forward Process

![image]({{site.baseurl}}/images/image-2022-07-01-18-49-19.png){:width="90%"}
<div class="figcap">Figure 2: Forward process of diffusion models.</div>

Given a data point sampled from a real data distribution $$\mathbf{x}_0 \sim q(\mathbf{x})$$, let us define a Markov chain, termed as **forward process** or **diffusion process**, in which we add small amount of Gaussian noise to the sample in $$T$$ steps, producing a sequence of noisy samples $$\mathbf{x}_1, \dots, \mathbf{x}_T$$. The step sizes are controlled by a variance schedule $$\{\beta_t \in (0, 1)\}_{t=1}^T$$.

$$
q(\mathbf{x}_{1:T}\vert \mathbf{x}_0) = \prod^T_{t=1} q(\mathbf{x}_t\vert \mathbf{x}_{t-1})\,,
$$

where

$$
q(\mathbf{x}_t\vert \mathbf{x}_{t-1})=\mathcal{N}(\mathbf{x}_t;\sqrt{1-\beta_t}\mathbf{x}_{t-1},\beta_t\mathbf{I})\,.
$$

By using **reparameterization trick**, we can derive:

$$
\mathbf{x}_t = \sqrt{1-\beta_t}\mathbf{x}_{t-1} + \sqrt{\beta_t}\boldsymbol{\epsilon}\,,\quad \boldsymbol{\epsilon}\sim\mathcal{N}(\mathbf{0},\mathbf{I})\,.
$$


The data sample $$\mathbf{x}_0$$ gradually loses its distinguishable features as the step $$t$$ becomes larger. Eventually when $$T\to\infty$$, $$\mathbf{x}_T$$ is equivalent to an isotropic Gaussian distribution.

A nice property of the above process is that we can sample $$\mathbf{x}_t$$ at any arbitrary time step $$t$$ in a closed form using **reparameterization trick**. Let $$\alpha_t=1-\beta_t$$ and $$\bar{\alpha}_t=\prod^t_{i=1}\alpha_i$$:

$$
q(\mathbf{x}_t\vert \mathbf{x}_0)=\mathcal{N}(\mathbf{x}_t;\sqrt{\bar{\alpha}_t}\mathbf{x}_0,(1-\bar{\alpha}_t)\mathbf{I})\,.
$$

Therefore,

$$
\mathbf{x}_t = \sqrt{\bar{\alpha}_t}\mathbf{x}_0 + \sqrt{1-\bar{\alpha}_t}\boldsymbol{\epsilon}\,.
$$

Usually, we can afford a larger update step when the sample gets noisier, so $$\beta_1 < \beta_2 < \dots < \beta_T$$ and therefore $$\bar{\alpha}_1 > \dots > \bar{\alpha}_T$$.

Suppose that $$\beta_t$$ is linear, the graphs of $$\beta_t$$, $$\bar{\alpha}_t$$ and $$1-\bar{\alpha}_t$$ are as follows.

![image]({{site.baseurl}}/images/image-2022-07-01-19-20-01.png){:width="70%"}
<div class="figcap">Figure 3: Graphs.</div>

We can observe that as the step $$t$$ increases, the weight of the data $$\mathbf{x}_0$$ gets lower while the weight of the noise $$\boldsymbol{\epsilon}$$ gets higher.


<!-- 
#### (1) Understanding $$q(\mathbf{x}_t\vert \mathbf{x}_{t-1})$$ and $$q(\mathbf{x}_t\vert \mathbf{x}_0)$$

The expression of $$q(\mathbf{x}_t\vert \mathbf{x}_{t-1})$$ is equivalent to

$$
\mathbf{x}_t = \sqrt{1-\beta_t}\mathbf{x}_{t-1} + \sqrt{\beta_t}\boldsymbol{\epsilon}\,,
$$

which denotes that the result of the current timestep $$\mathbf{x}_t$$ is a weighted average of the result of the previous timestep $$\mathbf{x}_{t-1}$$ and a Gaussian noise $$\boldsymbol{\epsilon}\sim\mathcal{N}(\mathbf{0},\mathbf{I})$$. While $$t\to\infty$$, $$\mathbf{x}_t$$ will become a pure Gaussian noise $$\boldsymbol{\epsilon}$$.

The next is the derivation of $$q(\mathbf{x}_t\vert \mathbf{x}_0)$$. Replacing $$1-\beta_t$$ with $$\alpha_t$$, we can get

$$
\mathbf{x}_1 = \sqrt{1-\alpha_1}\mathbf{x}_0 + \sqrt{1-\alpha_1}\boldsymbol{\epsilon}_0\,,
$$

$$
\begin{aligned}
\mathbf{x}_2 & = \sqrt{1-\alpha_2}\mathbf{x}_1 + \sqrt{1-\alpha_2}\boldsymbol{\epsilon}_1 \\
& = \sqrt{1-\alpha_2}(\sqrt{\alpha_1}\mathbf{x}_0+\sqrt{1-\alpha_1}\boldsymbol{\epsilon}_0) + \sqrt{1-\alpha_2}\boldsymbol{\epsilon}_1 \\
& = \sqrt{\alpha_1\alpha_2}\mathbf{x}_0 + (\sqrt{\alpha_2-\alpha_1\alpha_2}\boldsymbol{\epsilon}_0+\sqrt{1-\alpha_2}\boldsymbol{\epsilon}_1)\,.
\end{aligned}
$$

Since $$\boldsymbol{\epsilon}\sim\mathcal{N}(\mathbf{0},\mathbf{I})$$,

$$
\sqrt{\alpha_2-\alpha_1\alpha_2}\boldsymbol{\epsilon}_0 \sim \mathcal{N}(\mathbf{0}, (\alpha_2-\alpha_1\alpha_2)\mathbf{I})\,,
$$

$$
\sqrt{1-\alpha_2}\boldsymbol{\epsilon}_1 \sim \mathcal{N}(\mathbf{0}, (1-\alpha_2)\mathbf{I})\,.
$$

As the result of adding two Gaussian distribution is still a Gaussian distribution, we can get

$$
\sqrt{\alpha_2-\alpha_1\alpha_2}\boldsymbol{\epsilon}_0+\sqrt{1-\alpha_2}\boldsymbol{\epsilon}_1 \sim \mathcal{N}(\mathbf{0},(1-\alpha_1\alpha_2)\mathbf{I})\,,
$$

$$
\mathbf{x}_2 = \sqrt{\alpha_1\alpha_2}\mathbf{x}_0 + (\sqrt{1-\alpha_1\alpha_2}\boldsymbol{\epsilon})\,.
$$

By analogy,

$$
\mathbf{x}_t = \sqrt{\bar{\alpha}_t}\mathbf{x}_0 + \sqrt{1-\bar{\alpha}_t}\boldsymbol{\epsilon}\,.
$$

Therefore,

$$
q(\mathbf{x}_t\vert \mathbf{x}_0)=\mathcal{N}(\mathbf{x}_t;\sqrt{\bar{\alpha}_t}\mathbf{x}_0,(1-\bar{\alpha}_t)\mathbf{I})\,.
$$ -->


<!-- #### (2) Understanding Variance Schedule $$\beta_t$$

The role of $$\beta_t$$ is to control the weight allocation of data and noise during the forward process. Specifically, $$\beta_t$$ is related to $$\bar{\alpha}_t$$, and the relationship between them is

$$
\bar{\alpha}_t = \prod^T_{i=1}\alpha_i = \prod^T_{i=1}(1-\beta_i)\,.
$$

Besides, $$\sqrt{\bar{\alpha}_t}$$ and $$1-\sqrt{\bar{\alpha}_t}$$ can be seen as the weights of data $$\mathbf{x}_0$$ and noise $$\boldsymbol{\epsilon}$$ in the calculation of $$\mathbf{x}_t$$.

Normally, with the increase of timestep $$t$$, larger step size is used. That is to say, $$\beta_1<\beta_2<\cdots<\beta_T$$. Besides, we can easily derive $$\bar{\alpha}_1>\bar{\alpha}_2>\cdots>\bar{\alpha}_T$$.

Suppose that $$\beta_t$$ is linear, the graphs of $$\beta_t$$, $$\bar{\alpha}_t$$ and $$1-\bar{\alpha}_t$$ are as follows.

![image]({{site.baseurl}}/images/image-2022-07-01-19-20-01.png){:width="70%"}
<div class="figcap">Figure 2: Graphs.</div>

From the above graphs, we can observe that as $$t$$ increases, the weight of data gets higher while the weight of noise gets lower. -->


### 1.2. Reverse Process

![image]({{site.baseurl}}/images/image-2022-07-01-20-57-22.png){:width="90%"}
<div class="figcap">Figure 4: Reverse process of diffusion models.</div>

If we can reverse the above process and sample from $$q(\mathbf{x}_{t-1} \vert \mathbf{x}_t)$$, we will be able to recreate the true sample from a Gaussian noise input, $$\mathbf{x}_T \sim \mathcal{N}(\mathbf{0}, \mathbf{I})$$. Note that if $$\beta_t$$ is small enough, $$q(\mathbf{x}_{t-1} \vert \mathbf{x}_t)$$ will also be Gaussian. Unfortunately, we cannot easily estimate $$q(\mathbf{x}_{t-1} \vert \mathbf{x}_t)$$ because it needs to use the entire dataset and therefore we need to learn a model $$p_\theta$$ to approximate these conditional probabilities in order to run the **reverse process**.

$$
p_\theta(\mathbf{x}_{0:T}) = p(\mathbf{x}_T)\prod^T_{t=1}p_\theta(\mathbf{x}_{t-1}\vert \mathbf{x}_t)\,,
$$

where

$$
p_\theta(\mathbf{x}_{t-1}\vert \mathbf{x}_t) = \mathcal{N}(\mathbf{x}_{t-1}; \boldsymbol{\mu}_\theta(\mathbf{x}_t,t),\boldsymbol{\Sigma}_\theta(\mathbf{x}_t,t))\,.
$$

<!-- Similarly, we can derive

$$
\mathbf{x}_{t-1} = \boldsymbol{\mu}_\theta(\mathbf{x}_t,t) + \boldsymbol{\sigma}_\theta(\mathbf{x}_t,t) \cdot \boldsymbol{\epsilon}\,.
$$ -->

It is noteworthy that the reverse conditional probability is tractable when conditioned on $$\mathbf{x}_0$$:

$$
q(\mathbf{x}_{t-1} \vert \mathbf{x}_t, \mathbf{x}_0) = \mathcal{N}(\mathbf{x}_{t-1}; \tilde{\boldsymbol{\mu}}_t(\mathbf{x}_t, \mathbf{x}_0), \tilde{\beta}_t \mathbf{I})\,,
$$

where

$$
\tilde{\boldsymbol{\mu}}_t(\mathbf{x}_t, \mathbf{x}_0) = \frac{\sqrt{\bar{\alpha}_{t-1}} \beta_t}{1-\bar{\alpha}_t} \mathbf{x}_{0}+\frac{\sqrt{\alpha_{\iota}}(1-\bar{\alpha}_{
t-1})}{1-\bar{\alpha}_t} \mathbf{x}_t\,,
$$

$$
\tilde{\beta}_t = \frac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha}_t}\beta_t\,.
$$

Since $$\mathbf{x}_0 = \frac{1}{\sqrt{\bar{\alpha}_t}}(\mathbf{x}_t - \sqrt{1 - \bar{\alpha}_t}\boldsymbol{\epsilon})$$, we can plug it into $$\tilde{\boldsymbol{\mu}}_t(\mathbf{x}_t, \mathbf{x}_0)$$ and obtain:

$$
\tilde{\boldsymbol{\mu}}_t\left(\mathbf{x}_t, t\right)=\frac{1}{\sqrt{\alpha_t}}\left(\mathbf{x}_t-\frac{\beta_t}{\sqrt{1-\bar{\alpha}_t}} \boldsymbol{\epsilon}\right)\,.
$$


### 1.3. Training

The setup of diffusion models are very similar to VAE, so we can use the variational lower bound to optimize the negative log-likelihood $$-\log p_\theta(\mathbf{x}_0) $$. The final goal is to minimize the objective function $$L_\text{VLB}$$:

$$
L_\text{VLB}=\mathbb{E}_q [\underbrace{D_\text{KL}(q(\mathbf{x}_T \vert \mathbf{x}_0) \parallel p_\theta(\mathbf{x}_T))}_{L_T} + \sum_{t=2}^T \underbrace{D_\text{KL}(q(\mathbf{x}_{t-1} \vert \mathbf{x}_t, \mathbf{x}_0) \parallel p_\theta(\mathbf{x}_{t-1} \vert\mathbf{x}_t))}_{L_{t-1}} \underbrace{- \log p_\theta(\mathbf{x}_0 \vert \mathbf{x}_1)}_{L_0} ]\,.
$$

Every KL term in $$L_\text{VLB}$$ (except for $$L_0$$) compares two Gaussian distributions and therefore they can be computed in closed form. $$L_T$$ is constant and can be ignored during training because $$q$$ has no learnable parameters and $$\mathbf{x}_T$$ is a Gaussian noise. [Ho et al.](https://arxiv.org/abs/2006.11239) model $$L_0$$ using a separate discrete decoder derived from $$\mathcal{N}(\mathbf{x}_0; \boldsymbol{\mu}_\theta(\mathbf{x}_1, 1), \boldsymbol{\Sigma}_\theta(\mathbf{x}_1, 1))$$.

Recall that **we need to learn a neural network to approximate the conditioned probability distributions in the reverse diffusion process**, $$p_\theta(\mathbf{x}_{t-1} \vert \mathbf{x}_t) = \mathcal{N}(\mathbf{x}_{t-1}; \boldsymbol{\mu}_\theta(\mathbf{x}_t, t), \boldsymbol{\Sigma}_\theta(\mathbf{x}_t, t))$$. We would like to train $$\boldsymbol{\mu}_\theta$$ to predict $$\tilde{\boldsymbol{\mu}}_t = \frac{1}{\sqrt{\alpha_t}} \Big( \mathbf{x}_t - \frac{\beta_t}{\sqrt{1 - \bar{\alpha}_t}} \boldsymbol{\epsilon} \Big)$$. Because $$\mathbf{x}_t$$ is available as input at training time, we can reparameterize the Gaussian noise term instead to make it predict $$\boldsymbol{\epsilon}$$ from the input $$\mathbf{x}_t$$ at time step $$t$$:

$$
\boldsymbol{\mu}_\theta(\mathbf{x}_t, t) = \frac{1}{\sqrt{\alpha_t}} \Big( \mathbf{x}_t - \frac{\beta_t}{\sqrt{1 - \bar{\alpha}_t}} \boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t) \Big)\,.
$$

For simplification, $$\boldsymbol{\Sigma}_\theta(\mathbf{x}_t, t)$$ is set to untrained time dependent constants $$\sigma^2_t\mathbf{I}$$. Experimentally, both $$\sigma^2_t=\beta_t$$ and $$\sigma^2_t=\tilde{\beta}_t = \frac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha}_t}\beta_t$$ have similar results.

Therefore, the loss is parameterized to minimize the difference from $$\tilde{\boldsymbol{\mu}}_t$$
 :

$$
\begin{aligned}
L_t 
&= \mathbb{E}_{\mathbf{x}_0, \boldsymbol{\epsilon}} \Big[\frac{1}{2\sigma^2_t} \| \tilde{\boldsymbol{\mu}}_t(\mathbf{x}_t, \mathbf{x}_0) - \boldsymbol{\mu}_\theta(\mathbf{x}_t, t) \|^2 \Big] \\
&= \mathbb{E}_{\mathbf{x}_0, \boldsymbol{\epsilon}} \Big[\frac{ \beta_t^2 }{2 \sigma^2_t\alpha_t (1 - \bar{\alpha}_t)} \|\boldsymbol{\epsilon} - \boldsymbol{\epsilon}_\theta(\sqrt{\bar{\alpha}_t}\mathbf{x}_0 + \sqrt{1 - \bar{\alpha}_t}\boldsymbol{\epsilon}, t)\|^2 \Big] \,.
\end{aligned}
$$

Empirically, [Ho et al.](https://arxiv.org/abs/2006.11239) find that training the diffusion model works better with a simplified objective that ignores the weighting term, so the final simple objective is:

$$
L_\text{simple} = \mathbb{E}_{\mathbf{x}_0, \boldsymbol{\epsilon}} \Big[\|\boldsymbol{\epsilon} - \boldsymbol{\epsilon}_\theta(\sqrt{\bar{\alpha}_t}\mathbf{x}_0 + \sqrt{1 - \bar{\alpha}_t}\boldsymbol{\epsilon}, t)\|^2 \Big] \,.
$$

<!-- 
Training is performed by optimizing the usual variational bound on negative log likelihood:

Given $$\mathbf{x}_0$$, we hope that the probability $$p_\theta(\mathbf{x}_0)$$ predicted by the model can be as large as possible. The final goal function can be derived as 

$$
\min\ L=\mathbb{E}_q [\underbrace{D_\text{KL}(q(\mathbf{x}_T \vert \mathbf{x}_0) \parallel p_\theta(\mathbf{x}_T))}_{L_T} + \sum_{t=2}^T \underbrace{D_\text{KL}(q(\mathbf{x}_{t-1} \vert \mathbf{x}_t, \mathbf{x}_0) \parallel p_\theta(\mathbf{x}_{t-1} \vert\mathbf{x}_t))}_{L_{t-1}} \underbrace{- \log p_\theta(\mathbf{x}_0 \vert \mathbf{x}_1)}_{L_0} ]\,,
$$

where $${L_T}$$ is irrelevant to $$\theta$$, so it can be ignored. $$L_0$$ is the reconstruction term. $${L_{t-1}}$$ is the KL divergence that compares $$p_\theta(\mathbf{x}_{t-1}\vert \mathbf{x}_t)$$ against forward process posteriors which are tractable when conditioned on $$\mathbf{x}_0$$:

$$
q(\mathbf{x}_{t-1} \vert \mathbf{x}_t, \mathbf{x}_0) = \mathcal{N}(\mathbf{x}_{t-1};\tilde{\boldsymbol{\mu}}_t(\mathbf{x}_t,\mathbf{x}_0, t),\tilde{\beta}_t\mathbf{I})\,,
$$

where

$$
\tilde{\boldsymbol{\mu}}_t(\mathbf{x}_t,\mathbf{x}_0, t) = \frac{\sqrt{\bar{\alpha}_{t-1}}\beta_t}{1-\bar{\alpha}_t}\mathbf{x}_0 + \frac{\sqrt{\alpha}_t(1-\bar{\alpha}_{t-1})}{1-\bar{\alpha}_t}\mathbf{x}_t\,, \quad \tilde{\beta}_t = \frac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha}_t}\beta_t\,.
$$

Since 

$$
\mathbf{x}_t = \sqrt{\bar{\alpha}_t}\mathbf{x_0} + \sqrt{1 - \bar{\alpha}_t}\boldsymbol{\epsilon},
$$

we can get

$$
\mathbf{x_0} = \frac{\mathbf{x}_t-\sqrt{1 - \bar{\alpha}_t}\boldsymbol{\epsilon}}{\sqrt{\bar{\alpha_t}}}.
$$

We can further represent $$\tilde{\boldsymbol{\mu}}_t(\mathbf{x}_t,\mathbf{x}_0, t)$$ as

$$
\tilde{\boldsymbol{\mu}}_t\left(\mathbf{x}_t, t\right)=\frac{1}{\sqrt{\alpha_t}}\left(\mathbf{x}_t-\frac{\beta_t}{\sqrt{1-\bar{\alpha}_t}} \boldsymbol{\epsilon}\right).
$$




For simplification, we set $$\boldsymbol{\sigma}_\theta(\mathbf{x}_t,t) = \sigma^2_t$$, which are untrained time dependent constants. Experimentally, both $$\sigma^2_t=\beta_t$$ and $$\sigma^2_t=\tilde{\beta}_t = \frac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha}_t}\beta_t$$ have similar results.

Since $$\mathbf{x}_t$$ is available as input to the model, we may choose the parameterization

$$
{\mu}_{\theta}\left(\mathbf{x}_t, t\right)=\frac{1}{\sqrt{\alpha_t}}\left(\mathbf{x}_t-\frac{\beta_t}{\sqrt{1-\bar{\alpha}_t}} \boldsymbol{\epsilon}_\theta(\mathbf{x}_t,t)\right).
$$ -->

### 1.4. Sampling

After the model $$\boldsymbol{\epsilon}_\theta$$ is well-trained, we can use the following manner to sample the data.

$$
\begin{aligned}
\mathbf{x_{t-1}} 
&=\mu_{\theta}(\mathbf{x}_t,t) + \sigma_t\boldsymbol{\epsilon} \\
&=\frac{1}{\sqrt{\alpha_t}}\left(\mathbf{x}_t-\frac{\beta_t}{\sqrt{1-\bar{\alpha}_t}}\boldsymbol{\epsilon}_\theta(\mathbf{x}_t,t)\right)+ \sigma_t\boldsymbol{\epsilon} \\
&=\frac{1}{\sqrt{\alpha_t}}\left(\mathbf{x}_t-\frac{1-\alpha_t}{\sqrt{1-\bar{\alpha}_t}}\boldsymbol{\epsilon}_\theta(\mathbf{x}_t,t)\right)+ \sigma_t\boldsymbol{\epsilon}\,.
\end{aligned}
$$

In summary, the training and sampling algorithms are as follows.

<!-- ![image]({{site.baseurl}}/images/image-2022-07-02-02-12-39.png){:width="100%"}
<div class="figcap">Figure 5: Training and sampling algorithms</div> -->

![image]({{site.baseurl}}/images/image-2022-07-02-16-28-33.png){:width="100%"}
<div class="figcap">Figure 5: Training and sampling algorithms</div>

<br>


## 2. Experiments

For all experiments, $$T=1000$$, the forward process variances $$\beta_t$$ are set linearly from $$\beta_1=10^{-4}$$ to $$\beta_T=0.02$$.

The neural network of the reverse process is set to a U-Net backbone similar to an unmasked PixelCNN++ with group normalization throughout. Parameters are shared across time, and the time step is embedded into the network using the Transformer sinusoidal position embedding. Self-attention is also used at the $$16\times16$$ feature map resolution.

### 2.1. Sample Quality
The quantitative reuslts and some synthesized image samples are shown below. We can observe that DDPM achieves better sample quality than most models, inchluding class conditional models.

![image]({{site.baseurl}}/images/image-2022-07-02-13-56-06.png){:width="60%"}
<div class="figcap">Figure 6: Qualitative results on CIFAR 10.</div>

![image]({{site.baseurl}}/images/image-2022-07-02-13-57-57.png){:width="80%"}
<div class="figcap">Figure 7: Synthesized image samples on CelebA-HQ and CIFAR 10.</div>

![image]({{site.baseurl}}/images/image-2022-07-02-13-57-13.png){:width="100%"}
<div class="figcap">Figure 8: Synthesized image samples on LSUN.</div>

### 2.2. Training Objective Ablation

- The baseline option of predicting $$\tilde{\boldsymbol{\mu}}$$ works well only when
trained on the true variational bound instead of unweighted mean squared error. 
- Learning reverse process variances $$\boldsymbol{\Sigma}_\theta(\mathbf{x}_t,t)$$ leads to unstable training and poorer sample quality compared to fixed variances.
- Predicting $$\boldsymbol{\epsilon}$$ performs approximately as well as
predicting $$\tilde{\boldsymbol{\mu}}$$ when trained on the variational bound with fixed variances, but much better when trained with the simplified objective.

![image]({{site.baseurl}}/images/image-2022-07-02-14-04-06.png){:width="50%"}
<div class="figcap">Figure 9: Ablation study of loss functions.</div>


### 2.3. Progressive Generation

Random noises transform to images with fine details gradually over the course of the reverse process. 

![image]({{site.baseurl}}/images/image-2022-07-02-14-41-28.png){:width="100%"}
<div class="figcap">Figure 10: Unconditional CIFAR 10 progressive generation</div>

Starting from $$x_t$$ with larger $$t$$, the data is noiser and the information of original images is less, which leads to more multiple synthesized images.

![image]({{site.baseurl}}/images/image-2022-07-02-14-43-13.png){:width="100%"}
<div class="figcap">Figure 11: Synthesized results starting from different time steps.</div>


### 2.4. Interpolation

Compared with direct image space interpolation, conducting interpolation in latent space and decoding the linearly interpolated latent variable into image space generates more plausible and smoothly varying results.  

![image]({{site.baseurl}}/images/image-2022-07-02-14-44-34.png){:width="100%"}
<div class="figcap">Figure 12: Interpolations in latent space.</div>


<br>

## 2. Personal Thoughts

- When the time step $$t$$ is large, the loss between the predicted noise and the real noise should be small, because the input $$x_t$$ of the model is almost identical to the real noise. However, from an intuitive perspective, the reverse process maybe not in the "right" direction since there is little information of the original data in $$x_t$$ at this time. That is to say, starting from a random Gaussian noise, the direction of image synthesis can be arbitrary. This may be the reason why diffusion models can synthesize random images despite training with such a simple loss function.

- During training, the input $$x_t$$ is assigned the ground truth, but during inference, it is replaced with the predicted result of the previous step. This may cause error accumulation.

<br>

## 3. References

[1] Jascha Sohl-Dickstein et al., "[Deep Unsupervised Learning using Nonequilibrium Thermodynamics](https://arxiv.org/abs/1503.03585)", ICML 2015. 

[2] Jonathan Ho et al., "[Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2006.11239)", Advances in Neural Information Processing Systems 2020.
